Submitted August 31, 2023

2023 EU Systemic Risk Assessment

- Cover note -



Executive Summary



Wikipediaʼs inaugural 2023 EU Systemic Risk Assessment—in spreadsheet form—is appended tothis cover note. It sets out the Wikimedia Foundationʼs assessment of “systemic” risks, in theEU, linked to the use of Wikipedia. This is a legally-required document under the newEuropean Union Digital Services Act (DSA).

Out of 11 EU systemic risks that were assessed, the 2023 risk assessment designates threeimmediate priorities:1. Disinformation around conflicts, and/or civic and electoral processes2. Disinformation around historical/geographical narratives3. Harassment of Wikipediaʼs volunteer users

The Foundation is now documenting a range of existing and planned mitigations for all 11 risks.Once complete, the mitigations plan will also be submitted to regulators—as a follow up to thisRisk Assessment—without delay. This will be repeated at least once a year.

Globally, the Foundation deals with a much wider range of risks—such as Wikimedia projectsbeing blocked in some non-EU countries. Despite those global risksʼ

 importance, weunderstand them to be out of scope for this DSA-specific exercise.



The Wikimedia Foundationʼs 2023-24 Annual Plan

 provides a more holistic overview of theissues our volunteer editors and audience of readers are facing every day, and what is beingdone to protect and defend the rights of our audiences and editors to access and share freeknowledge across borders. Furthermore, the Foundationʼs

 Human Rights Policy describes theorganizationʼs commitment to identify and mitigate human rights risks globally.



About the Wikimedia Foundation

The Wikimedia Foundation is the nonprofit organization that hosts

 Wikipedia and other freeknowledge projects. The vision of our free knowledge communities is a world in which everysingle human being can freely share in the sum of all knowledge. To this end, we support avibrant community of more than 300,000 volunteers around the world, who contribute to the

1

Wikimedia projects by adding, editing, and verifying content in over 55 million articles acrossmore than 300 languages, all for free and without ads.



Contents

Executive Summary 1About the Wikimedia Foundation 1Background 2

About this document 2About the Wikimedia model: advantages and challenges when it comes to EU DSA SRAM 4How are relevant risks selected? 6What risks are excluded by this approach? 7How are the selected risks assessed? 8

What are the key risks highlighted in this 2023 exercise? 9Conclusion 10



Background

About this document



Wikipediaʼs inaugural 2023 EU Systemic Risk Assessment sets out the Wikimedia Foundationʼsassessment of systemic risks linked to the use of Wikipedia in the EU. This is a legally-requireddocument under the new DSA. It has been completed by the Foundationʼs Legal department inconsultation with a range of internal subject matter experts and stakeholders across theFoundation.

The Risk Assessment is an intermediate step in a wider exercise. The DSAʼs overall SystemicRisk Assessment and Mitigation (SRAM) process requires the Foundation (as the hostingprovider of Wikipedia) to not only assess risks, but also determine what risk mitigations areappropriate.

This process of assessment and mitigation is repeated at least annually. Interim updates maybe appropriate, in response to significant changes in the risk landscape.

2

Figure 1: Diagram showing the stages of the EU DSA SRAM process.



The Foundation is opting for a single, living SRAM document which can be updated on anongoing basis. This is also sometimes referred to as the Foundationʼs DSA Risk Register. Eachyear, an annual snapshot will be taken of that document, and will be filed with regulators.

The DSA sets out risk assessment and mitigation as a two-step process (requiring two successivefilings: first the risk assessment, and then the mitigation plan—see Figure 1, above). Despitethis, our DSA SRAM Register has been structured so that it can be used in a single-step strategyin future years if appropriate (see Figure 2, below).

For the first year, the Register will first be submitted to the European Commission without theMitigations content, while we assess and plan our mitigations in response to the assessed(submitted) risks. Once the mitigation planning is complete, a consolidated document (i.e.including the final column, for Mitigations) will be submitted.

3

Figure 2: Example extract from the Foundationʼs EU DSA SRAM Register. Its components will beupdated and filed at least once a year with EU regulators. The DSA foresees the Mitigations column(blue) being completed and filed separately, as a second step following Risk Assessment. In practice,our DSA SRAM Register—once complete, i.e. from 2024 onwards —will record both our assessment ofrisks and their mitigations; this example, which shows all columns, illustrates our planned end-statefor the Register.



About the Wikimedia model: advantages and challenges when it comes to EUDSA SRAM



Wikipedia and other Wikimedia projects provide free access to neutral, well-sourcedinformation about science, culture, history, and other encyclopedic subjects. The informationin question is added, organized, and edited by a decentralized community of volunteers whoengage in open debate to reach consensus around content decisions and policies.

Volunteers address most everyday content issues on the Wikimedia projects, such asintentional vandalism or edits that do not meet Wikipediaʼs reliability and neutrality standards,without interference from the Wikimedia Foundation.

This volunteer-led, self-governing model provides for several advantages that allow ourmovement to advance knowledge equity and to combat disinformation through collectivecontributions and open debate about content and content moderation. A global community ofcommitted volunteers with specific expertise and local knowledge and language skills havebuilt an unprecedented repository of encyclopedic information that is available and culturallyrelevant to more than 300 language communities around the globe. These same qualities allowthese communities to monitor Wikipedia pages for disinformation, and to rapidly removecontent that may range from common vandalism to artifacts of coordinated disinformationcampaigns. All of this work adding, editing, and verifying content takes place in the open and isgoverned by policies developed and implemented democratically by these volunteercommunities themselves.

4

This unique model has allowed the Wikimedia projects to achieve the prominence and highquality they have now, and the volunteer-led, distributed decision-making that occurs on theWikimedia projects is a manifestation of the ideals of online participation and freedom ofexpression.

Despite these advantages, some challenges naturally arise.

One challenge relevant to this work emanates from our community governance model. Beyondthe Foundationʼs Terms of Use and other high-level policies, most policies and decision makingrelated to platform governance and content moderation are developed and implemented bythese volunteer communities. Thus, any significant changes to platform governance andcontent moderation policies or practices—such as possible recommendations stemming fromhuman rights impact assessments—require consultation with, buy-in from, and leadership bythese communities. This model of governance, therefore, can be slower to effect change thanthat which is feasible for platforms with top-down models.

Another challenge is that the Wikimedia Foundation operates as a non-profit organization. TheFoundationʼs largest share of revenues comes from individual donations; it does not generate adrevenue or sell user data to generate income. As a result of this model, the Foundation hasfewer financial and human resources available to tackle significant technical changes orupgrades than do other VLOPs. Any reallocation of resources to make significant changes toWikipedia for risk mitigation requires pulling resources from other priority areas, such asrequested feature development, fixing bugs or broken tools, enhancing the overall stability andreliability of the websites we host, or—more fundamentally—continuing to create anenvironment in which volunteers want to engage and improve the projects.

This presents a challenge with respect to the EU DSA SRAM process: the DSA expects greaterintervention from platform operators (“accountability” / “ responsibilization” of platforms).This is a sensible objective for more conventional, for-profit, social media platforms, whosecontent policies and moderation are imposed on a top-down basis. However, when it comes tothe Wikimedia projects, regulatory obligations should not be interpreted in a way that reducesthe Wikimedia communitiesʼ autonomy, enthusiasm, and control.

Accordingly, in stark contrast to other commercial platforms, the Foundation intends togenerally refrain from dictating changes to content policy, or displacing effective communitymechanisms (e.g. for complaint handling, or efforts to tackle certain categories of problematiccontent). Our focus is instead on creating the right conditions for success. This is achieved bycontinuing the Foundationʼs catalytic, framework role, for instance by creating useful tools,supporting community structures (such as key committees), offering

 training, fostering

5

discussion, offering guidance and support, and engaging in the co-creation of new policies andsystems, such as the Universal Code of Conduct and its enforcement processes.

The focus is therefore on ensuring that the Wikipedia communities have the means to tacklethese important problems—rather than usurping their leading role.



How are relevant risks selected?



Our starting point is the Foundationʼs global, non-DSA-specific view of the risk landscape forWikimedia projects. In particular, our DSA exercise builds on the earlier results of a detailed,third-party-led, enterprise-wide Human Rights Impact Assessment (HRIA), last conducted in2020, which laid out a detailed analysis of risks relating to the Wikimedia projects generally, andhow they could be better mitigated. This is now being complemented by follow-up work, suchas a Child Rights Impact Assessment, assessments related to the use and deployment of specifictechnologies such as machine learning, and the integration of routine due diligence processesinto work streams across the organization.

The DSA is clear that the Foundationʼs global, non-DSA-specific work should not be used as-isfor this specific exercise.

 For one thing, the HRIA (and related work) are not EU-specific. Foranother, not all risks in the HRIA have the EU-wide scope and scale to be “systemic”, as the DSArequires. And not all of them are also Wikipedia-specific.1 For example, the use of algorithmsto recommend content on commercial platforms has resulted in significant concern among thegeneral public around the privacy of usersʼ data and how that data feeds into those algorithms,the internal workings of which are rarely visible to the public. On Wikipedia, however, the fewalgorithms used to recommend encyclopedic articles do not rely heavily on individualsʼ userdata because Wikipedia only collects the minimal amount of user data feasible. Furthermore,the internal workings of such algorithms are documented transparently using model cards forthe public to scrutinize.

These differences represent an important distinction between how commercial platforms andWikipedia use these technologies – a difference that is often underappreciated by the generalpublic. We are therefore being careful to assess the seriousness of each risk identified in ourrisk register based on two factors: 1) how that risk could manifest on Wikipedia given how theplatform actually operates; and 2) the likelihood of that risk arising within the EU.



1 Of all the Wikimedia projects, only Wikipedia is in-scope for the DSA SRAM exercise—it is the onlyproject large enough to have been designated as a Very Large Online Platform (VLOP); the otherWikimedia projects will be subject to several other DSA obligations, but not SRAM.



6

Based on the wording and structure of the law, our first step for this SRAM exercise wastherefore to select risks by taking the global risks that were highlighted in our HRIA (andrelated work), and then applying DSA-specific filters:



Figure 3: Diagram showing how the Wikimedia Foundation selects risks for inclusion in the DSASRAM process.



Based on the four factors shown in Figure 3, the global risks and their assessments are—or arenot—transposed (modified, if appropriate) into the DSA SRAM Register.

 For the most part, theFoundationʼs broader assessments of global risks, e.g. the analysis contained in our HRIA, willremain somewhat relevant to the risks transposed into the DSA SRAM Register. Thosedocuments therefore provide important further reading for those with an interest in thevarious risks logged in the DSA SRAM Register.



What risks are excluded by this approach?



DSA-specific filtering means that important global risks, which are the focus of extensiveFoundation and wider community activity, are not reflected in the DSA SRAM Register.

For example, country-wide blocking of the Wikimedia projects is a major risk for affectedpopulations and for the Wikimedia projects themselves, which then have reduced participationfrom affected citizens.

 We do not currently consider this an EU systemic risk, nor is it clearlylinked to Wikipediaʼs design/functioning/use, so it has not been included in this DSA-specificexercise, despite its wider importance.

7

In addition, it should be noted that Wikipedia was designated as a VLOP based on its number ofmonthly active visitors from the EU. However, certain of the risks included in our DSA SRAMexercise are likely to be experienced only by actively-engaged volunteers, such as those makingand discussing changes to Wikipedia articles. This is a much smaller number of people—farbelow the 45 million user VLOP threshold. By way of example, English-language Wikipediaonly sees (approximately) 59,000 active editors a month; of these, at least 34,000 come fromnon-EU countries.2 For French Wikipedia, there are only approximately 5,700 active editorsfrom France, and another 420 from Belgium. Accordingly, even though the Foundation isfocused on tackling risks such as volunteer harassment, globally, it was unclear to us to whatextent that should be selected as a DSA-specific EU systemic risk.

 Future iterations of the EUDSA SRAM exercise may omit it, even though it was included for 2023. Similar considerationsapply for risks that might concern an even smaller population of users, such as active EUeditors aged under 18.



How are the selected risks assessed?



Starting from our existing (wider) assessment of risks in exercises such as our HRIA, we thenconduct a second pass assessment from a DSA-specific perspective.

Our primary consideration is (1) the likelihood of a risk manifesting, and (2) the impact a risk islikely to have, if it manifests. Among other factors, this takes into account the mitigationswhich are already in place.

The distribution of Child Sexual Abuse Material (CSAM), for instance, can be damaging, but itsprevalence on Wikipedia (as an EU systemic risk) appears to be low. Unlike other websites, theopenness of Wikipedia, i.e. that all contributions and edits are logged publicly, and volunteereditors can identify and remove such content, makes it a highly unattractive platform fordistributing such material. That transparency similarly makes Wikipedia a relativelyunattractive environment for grooming, or other types of conduct that thrive in obscurity.

Finally, our risk prioritization (including the ability to devote resources to a selected riskʼsmitigation, for DSA purposes) also takes into account the Foundation and wider editor andreader communitiesʼ global (non-DSA-specific) priorities. As noted above, the Wikimediaprojects, globally, are exposed to important risks, such as mass censorship, or impediments to



2

https://stats.wikimedia.org/#/en.wikipedia.org/contributing/active-editors-by-country/normal|table|last-month|(activity-level)~5..99-edits|monthly



8

global knowledge equity, that demand significant prioritization outside of this EU, DSA-specificexercise.

For all these reasons, it is important to also note that the results of the Foundationʼs 2023 DSASRAM exercise, including this inaugural Systemic Risk Assessment, cannot be compared on alike-for-like basis with those produced by other platforms and search engines that have beendesignated as VLOPs and VLOSEs. For instance, the number of EU-based users exposed tosome of these risks on large social media platforms could be orders of magnitude greater thanon Wikipedia.



What are the key risks highlighted in this 2023 exercise?



Our assessment for 2023 identifies three immediate priorities:

1. Disinformation regarding civic and electoral processes, and conflicts2. Disinformation regarding historical/geographical narratives3. Harassment among the volunteer community

Work on these is already underway across the whole communities of Wikipedia contributorsand allies. More generally, Wikipedia is a broadly welcoming environment for contributors thatresults in reliable, neutrally-presented information on a vast range of topics. This is the resultof longstanding efforts by the community, the Foundation, and others. But as is clear not justfrom our HRIA, but also community conversations and outside research, more can be done.

Although the full range of mitigations is yet to be fully documented and assessed for 2023, it ispossible to preview the main ongoing mitigations for each of these risks:

1. Anti-harassment efforts

 are being supported by rollout of the Universal Code ofConduct and its related guidance and enforcement processes; and the Foundation isworking on a new, supplemental way for UCoC-violating incidents to be reported;

2. Anti-disinformation efforts

 are sufficiently diverse that the Foundation has developedan overarching Anti-Disinformation Strategy. Specific

 disinformation risks—forexample ahead of certain elections—are responded to by supporting the formation oftemporary task forces (by the volunteer community, or the Foundation, or both). Thisapproach is backed by a program of work to improve technical tools to supportmoderation and content curation carried out by volunteer editors, and a wider researchprogram to support Knowledge Integrity on Wikipedia and its sister projects. There are

9

also plans to develop an e-learning module to assist volunteer editors in identifying andcombatting disinformation.

Other risks that were assessed and are being mitigated, as part of this exercise, include thefollowing. More details on these risks and mitigations can be found in the Risk Register itself,and the underlying materials it is based on.



Medium-Term Priorities:

1. Disinformation regarding scientific information and conspiracy theories2. Dangerous content: Exposure of vulnerable individuals to content that may bedangerous in that context3. Child sexual abuse material (CSAM)4. Terrorist or violent extremist content (TVEC)



Long-Term Priorities:

5. Over-reliance on moderation tech: Possible detrimental side-effects of the increased useof technological tools for content moderation on Wikipedia, in particular (i) algorithmicpropagation of bias, and/or (ii) over-reliance on tools, that could become detrimental tovolunteer skills and participation6. Propagation of disinformation or bias through generative AI7. Privacy risks for young contributors8. Attacks on individuals profiled on Wikipedia



Conclusion

The Wikimedia Foundation appreciates the opportunity to discuss these risks with theEuropean Commission, members of the volunteer community that make free knowledgeprojects like Wikipedia possible, and, indeed, the broader public. The Foundation looksforward to engaging in sustained dialogue with these stakeholders around these risks, how it isalready working to mitigate them now and in the future, and how our work to identify andmitigate risks will continue in the coming years as the digital ecosystem evolves.

This process of systemic risk assessment has served as a timely incentive for the Foundation totake stock of the broader societal impacts of Wikipedia within the EU. We welcome theopportunity to describe to the Commission, and the EU citizens it represents, how theorganization plans to mitigate risks that the DSA is intended to address. Indeed, we welcomeefforts by governments to work with platforms on processes that foster greater transparencyand accountability for online platforms.

10

As discussed above, many important actions to mitigate the risks identified in this submissionwill require buy-in and action from Wikimedia affiliates and volunteers throughout the EU. Wehope our publication of material about the risks and mitigations will ultimately spark importantdiscussions with the Wikimedia communities about their important and essential roles inmitigating risks to themselves and European society more broadly, thereby empoweringWikimedians across Europe to exercise leadership and initiative in our collective endeavor.



\*



11